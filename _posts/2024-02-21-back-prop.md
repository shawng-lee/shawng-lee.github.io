---
layout: post  
title: "Understanding Backpropagation"
categories: [Back Propagation, Model Training, Deep Learning, Neural Networks]  # Optional - add categories if you like
---
# Understanding Backpropagation
When I first heard the term 'backpropagation' it was on Day 1 of my Deep Learning & Bayesian analytics subject at my university. The professor asked the class of computer scientists, data scientists, physicists, economists, and finance majors if anyone was familiar with term and the room was dead silent in response. Suffice to say, backpropagation is not the hottest buzzword circulating the techsphere and yet it serves as the backbone in training modern neural networks.

## The phases of Deep Learning
Neural networks facilitate 'learning' in two phases: forward propagation and backpropagation. Let's talk about forward propagation first. Forward propagation is when the network ingests a set of input data, runs it through different weighted sums and activation functions, before finally providing a prediction as the output. You can liken this to typing a prompt in ChatGPT, seeing a brief loading animation, and eventually receiving the intro to your blog post as the output (I'm kidding, of course-- I used Gemini for today).

That is a very high-level way of putting it and we can definitely dive deeper into the complexities of feed-forward networks, but forward propagation is not where the magic of learning happens. You see, if forward propagation were the only component to deep learning, then you *any* model that can take inputs and produce a prediction would be called a deep learning model. Instead, a neural network *learns* when it takes the prediction from forward propagation, compares it to the actual result it needed to predict, and updates all the weights it used during the forward propagation phase.

## What actually happens when a neural network learns
Because we've described things in such high-level, you may think there are layers upon layers of complex, black box computations to perform such a task. After all, that's why Deep Learning only emerged in the last few years, right? It turns out, Deep Learning and backpropagation have been around since the 80's and the math behind it is fairly modest!
